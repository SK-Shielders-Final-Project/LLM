## vLLM 서버 설치/실행 (Linux 일반)

# 1) Python 환경 (dnf 기준)
# - RHEL/CentOS/Rocky:
#   sudo dnf install -y python3 python3-venv python3-pip
python3 -m venv venv
source venv/bin/activate

# 3) vLLM 설치 (CUDA 환경 필요)
pip install --upgrade pip
pip install vllm

# 4) 모델 캐시 경로(선택)
export HF_HOME=/data/hf_cache

# 5) vLLM 서버 실행 (OpenAI 호환 API)
vllm serve "Qwen/Qwen2.5-3B-Instruct-AWQ" \
  --quantization awq_marlin \
  --max-num-seqs 8 \
  --max-model-len 4096 \
  --gpu-memory-utilization 0.80 \
  --host 0.0.0.0 --port 8001

## 방화벽/보안그룹에서 8001 포트 허용 필요



vllm serve "Qwen/Qwen2.5-3B-Instruct" \
  --enable-auto-tool-choice \
  --tool-call-parser pythonic \
  --max-num-seqs 1 \
  --max-model-len 1024 \
  --gpu-memory-utilization 0.80 \
  --host 0.0.0.0 --port 8001

vllm serve RedHatAI/gemma-3-12b-it-FP8-dynamic \
  --dtype bfloat16 \
  --trust-remote-code \
  --max-model-len 4096 \
  --gpu-memory-utilization 0.80 \
  --host 0.0.0.0 --port 8001